# -*- coding: utf-8 -*-
"""Make_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pS-ZtV_97dMUsSuEUNRwoJ2euECCd6Zd
"""

import numpy as np
import pandas as pd
import sklearn.feature_extraction.text as fe
import re
import gc
import collections
import os
import random
import urllib.request
from pprint import pprint

# Load in corpus. Sanitize both table and text.
data = pd.read_csv('NewsArticles.csv', header=None, encoding='unicode_escape')
data.columns = data.iloc[0]
data = data[1:]
data = data[['title', 'text']].rename(columns={'title': 'title', 'text': 'body'})

# Turn words to lowercase and remove anything that isn't a letter.
def pre_process(text):
    text=text.lower()
    text=re.sub("</?.*?>"," <> ",text)
    text=re.sub("'","",text) #e.g. i'm -> im
    text=re.sub("(\\d|\\W)+"," ",text)
    return text

data['text'] = data['title'] + data['body'] #generate a new column 'text' combining both 'title' and 'body'
test = data[len(data)-200:]
data = data[:len(data)-200]
data['text'] = data['text'].apply(lambda x:pre_process(str(x)))

# Tokenize text using some gimick from sklearn
cv=fe.CountVectorizer(max_features=50000,max_df=0.75,min_df=0.0005)
wcv=cv.fit_transform(data['text'].to_list())
names=cv.get_feature_names_out()
vocab = set(names)
def tokenize(text):
  tokens = []
  for i in text.split():
    if i in vocab:
      tokens.append(i)
    if i == '<s>':
      tokens.append(i)
  return tokens
data['tokens'] = data['text'].apply(lambda x:tokenize(x))

from gensim.models import Word2Vec

# Using gensim, the embedding can be easily trained
model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)
model.wv.save("word2vec.kvmodel") #the word embedding
